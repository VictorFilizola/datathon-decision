{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d06231",
   "metadata": {},
   "source": [
    "# Splitting Large JSON Files: Explanation and Demonstration\n",
    "\n",
    "Large JSON files can be difficult to process due to memory constraints, slow loading times, or the need to parallelize data processing. Splitting these files into smaller parts makes them easier to handle, share, and process in data pipelines or machine learning workflows.\n",
    "\n",
    "This notebook explains and demonstrates the logic behind the `file_splitter.py` script, which splits a large JSON file into two smaller files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e708e7",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We use the `json` module to read and write JSON files in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6854969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f85502c",
   "metadata": {},
   "source": [
    "## 2. Load the Input JSON File\n",
    "\n",
    "We start by reading the input JSON file and loading its contents into a Python variable. The file can contain either a list or a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa9ba16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Example input file path (update as needed)\n",
    "input_file = '../data/processed/applicants_for_processing_part6.json'\n",
    "\n",
    "# Load the JSON data\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Show the type of data loaded\n",
    "print(f\"Loaded data type: {type(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a36d0f",
   "metadata": {},
   "source": [
    "## 3. Determine JSON Structure and Split Data\n",
    "\n",
    "Depending on whether the data is a list or a dictionary, we split it into two parts. For lists, we slice it in half. For dictionaries, we split the keys and create two new dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e85211d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict split: 2933 keys in part1, 2933 keys in part2\n"
     ]
    }
   ],
   "source": [
    "# Split the data into two parts\n",
    "if isinstance(data, list):\n",
    "    half = len(data) // 2\n",
    "    part1 = data[:half]\n",
    "    part2 = data[half:]\n",
    "    print(f\"List split: {len(part1)} items in part1, {len(part2)} items in part2\")\n",
    "elif isinstance(data, dict):\n",
    "    keys = list(data.keys())\n",
    "    half = len(keys) // 2\n",
    "    keys1 = keys[:half]\n",
    "    keys2 = keys[half:]\n",
    "    part1 = {k: data[k] for k in keys1}\n",
    "    part2 = {k: data[k] for k in keys2}\n",
    "    print(f\"Dict split: {len(part1)} keys in part1, {len(part2)} keys in part2\")\n",
    "else:\n",
    "    raise ValueError(\"Unsupported JSON structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3647e6",
   "metadata": {},
   "source": [
    "## 4. Write Split Data to Output Files\n",
    "\n",
    "We save each part of the split data into separate output JSON files using `json.dump()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28f0a757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into ../data/processed\\applicants_for_processing_part11.json and ../data/processed\\applicants_for_processing_part12.json\n"
     ]
    }
   ],
   "source": [
    "input_file = '../data/processed/applicants_for_processing.json'\n",
    "import itertools\n",
    "\n",
    "# Get the directory and base name of the input file\n",
    "input_dir = os.path.dirname(input_file)\n",
    "input_base = os.path.splitext(os.path.basename(input_file))[0]\n",
    "\n",
    "# Find next available part numbers to avoid overwriting\n",
    "def get_next_available_filename(base, part, ext, directory):\n",
    "    for i in itertools.count(part):\n",
    "        candidate = os.path.join(directory, f'{base}_part{i}{ext}')\n",
    "        if not os.path.exists(candidate):\n",
    "            return candidate, i\n",
    "\n",
    "output_file1, part_num1 = get_next_available_filename(input_base, 1, '.json', input_dir)\n",
    "output_file2, part_num2 = get_next_available_filename(input_base, part_num1+1, '.json', input_dir)\n",
    "\n",
    "with open(output_file1, 'w', encoding='utf-8') as f1:\n",
    "    json.dump(part1, f1, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(output_file2, 'w', encoding='utf-8') as f2:\n",
    "    json.dump(part2, f2, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Split into {output_file1} and {output_file2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b77f8",
   "metadata": {},
   "source": [
    "## 5. Verify Output Files\n",
    "\n",
    "Finally, we read back the output files and check the number of items in each to confirm the split was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab9bf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1: 2933 keys\n",
      "Part 2: 2933 keys\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with open(output_file1, 'r', encoding='utf-8') as f1:\n",
    "    part1_loaded = json.load(f1)\n",
    "\n",
    "with open(output_file2, 'r', encoding='utf-8') as f2:\n",
    "    part2_loaded = json.load(f2)\n",
    "\n",
    "if isinstance(part1_loaded, list):\n",
    "    print(f\"Part 1: {len(part1_loaded)} items\")\n",
    "    print(f\"Part 2: {len(part2_loaded)} items\")\n",
    "elif isinstance(part1_loaded, dict):\n",
    "    print(f\"Part 1: {len(part1_loaded.keys())} keys\")\n",
    "    print(f\"Part 2: {len(part2_loaded.keys())} keys\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
